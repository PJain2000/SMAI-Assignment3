{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.4.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                        ])\n",
    "\n",
    "train_set = datasets.MNIST('./Dataset-3', download=True, train=True, transform=transform)\n",
    "val_set = datasets.MNIST('./Dataset-3', download=True, train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.view(data.shape[0], -1)\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    pred_arr = []\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).data.item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        for i in numpy.array(pred):\n",
    "            pred_arr.append(int(i))\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(val_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset), accuracy))\n",
    "    \n",
    "    return pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309464\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.907003\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.376824\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.276851\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.345412\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.304346\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.207404\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.338337\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.520478\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.228887\n",
      "\n",
      "Validation set: Average loss: 0.2728, Accuracy: 9211/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.250406\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.558774\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.287441\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.353965\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.112871\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.270819\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.485234\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.133416\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.115772\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.180388\n",
      "\n",
      "Validation set: Average loss: 0.2171, Accuracy: 9343/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.403647\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.063474\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.304317\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.082626\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.156674\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.280566\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.082290\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.044991\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.423108\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.108776\n",
      "\n",
      "Validation set: Average loss: 0.1633, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.026563\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.201686\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.101088\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.119784\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.160938\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.250833\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.149968\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.036563\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.207013\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.153458\n",
      "\n",
      "Validation set: Average loss: 0.1290, Accuracy: 9596/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.145281\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.117787\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.091086\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.278759\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.102493\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.115219\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.096556\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.070058\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.094554\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.047594\n",
      "\n",
      "Validation set: Average loss: 0.1228, Accuracy: 9620/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.062618\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.082642\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.101368\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.072434\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.076895\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.063236\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.181962\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.325390\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.040160\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.197853\n",
      "\n",
      "Validation set: Average loss: 0.1244, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.054344\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.006310\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.206730\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.179749\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.174143\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.046840\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.045654\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.030797\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.064237\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.028015\n",
      "\n",
      "Validation set: Average loss: 0.0988, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.044861\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.122501\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.073936\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.010186\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.186015\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.136657\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.092304\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.088638\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.100790\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.061675\n",
      "\n",
      "Validation set: Average loss: 0.0932, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.144328\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.006296\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.073050\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.162591\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.005200\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.077137\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.091312\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.061812\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.043815\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.035309\n",
      "\n",
      "Validation set: Average loss: 0.1006, Accuracy: 9687/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.111077\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.202822\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.093402\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.059557\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.042134\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.011308\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.117342\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.122974\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.009525\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.094501\n",
      "\n",
      "Validation set: Average loss: 0.0835, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "CPU times: user 5min 28s, sys: 2.76 s, total: 5min 31s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    output = validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                        ])\n",
    "\n",
    "train_set = datasets.MNIST('./Dataset-3', download=True, train=True, transform=transform)\n",
    "val_set = datasets.MNIST('./Dataset-3', download=True, train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=200):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "#         data = data.view(data.shape[0], -1)\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    pred_arr = []\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "    #         data = data.view(data.shape[0], -1)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += F.nll_loss(output, target, reduction='sum').data.item()\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            for i in numpy.array(pred):\n",
    "                pred_arr.append(int(i))\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "\n",
    "    accuracy = 100. * correct.to(torch.float32) / len(val_loader.dataset)\n",
    "    accuracy_vector.append(accuracy)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset), accuracy))\n",
    "    \n",
    "    return pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316251\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.774800\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.935153\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.823217\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.536507\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.756309\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.388988\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.665120\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.307832\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.309558\n",
      "\n",
      "Validation set: Average loss: 9.0548, Accuracy: 9218/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.373117\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.418231\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.453074\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.209779\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.334920\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.189587\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.460681\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.369078\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.396763\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.557036\n",
      "\n",
      "Validation set: Average loss: 7.2989, Accuracy: 9358/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.387455\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.323924\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.226513\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.217060\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.309019\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.469447\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.720076\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.223923\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.353860\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.205025\n",
      "\n",
      "Validation set: Average loss: 6.6882, Accuracy: 9408/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.263693\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.173094\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.138790\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.179758\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.308867\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.247557\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.132361\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.609470\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.676287\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.476916\n",
      "\n",
      "Validation set: Average loss: 6.3900, Accuracy: 9421/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.345302\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.212476\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.196158\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.187544\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.594840\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.121919\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.247066\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.275591\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.677886\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.096251\n",
      "\n",
      "Validation set: Average loss: 6.2798, Accuracy: 9430/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.088760\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.214768\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.159037\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.623503\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.247398\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.247777\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.115102\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.454757\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.223141\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.187568\n",
      "\n",
      "Validation set: Average loss: 6.2245, Accuracy: 9439/10000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.354025\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.074296\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.092984\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.226674\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.391379\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.284148\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.061916\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.455562\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.660948\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.252726\n",
      "\n",
      "Validation set: Average loss: 6.1971, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.209971\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.337677\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.348093\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.176659\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.384126\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.184159\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.178125\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.112492\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.265284\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.483693\n",
      "\n",
      "Validation set: Average loss: 6.1784, Accuracy: 9444/10000 (94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.161143\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.297830\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.516077\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.363825\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.132772\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.142508\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.244857\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.210749\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.343448\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.174268\n",
      "\n",
      "Validation set: Average loss: 6.1696, Accuracy: 9440/10000 (94%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.131489\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.312594\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.088218\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.870516\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.364307\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.085118\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.342535\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.098029\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.260434\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.382975\n",
      "\n",
      "Validation set: Average loss: 6.1661, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "CPU times: user 1h 1min 15s, sys: 2min 20s, total: 1h 3min 36s\n",
      "Wall time: 22min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    output = validate(lossv, accv)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                        ])\n",
    "\n",
    "train_set = datasets.MNIST('./Dataset-3', download=True, train=True, transform=transform)\n",
    "val_set = datasets.MNIST('./Dataset-3', download=True, train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set)\n",
    "val_loader = torch.utils.data.DataLoader(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_val = []\n",
    "y_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_loader:\n",
    "    img = numpy.array(numpy.reshape(x, (28, 28))).flatten()\n",
    "    x_train.append(img)\n",
    "    y_train.append(int(y))\n",
    "    \n",
    "for x,y in val_loader:\n",
    "    img = numpy.array(numpy.reshape(x, (28, 28))).flatten()\n",
    "    x_val.append(img)\n",
    "    y_val.append(int(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = numpy.array(x_train)\n",
    "y_train = numpy.array(y_train)\n",
    "\n",
    "x_val = numpy.array(x_val)\n",
    "y_val = numpy.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model\n",
    "\n",
    "model_linear = SVC(C= 0.85, kernel='poly')\n",
    "model_linear.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model_linear.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9827 \n",
      "\n",
      "[[ 973    0    2    0    0    2    0    1    2    0]\n",
      " [   0 1131    1    1    0    1    0    1    0    0]\n",
      " [   5    1 1014    0    1    0    1    6    4    0]\n",
      " [   0    0    2  994    0    2    0    4    5    3]\n",
      " [   0    0    4    0  967    0    2    0    0    9]\n",
      " [   3    0    0    7    1  874    3    0    2    2]\n",
      " [   5    2    0    0    2    5  942    0    2    0]\n",
      " [   1    7    8    3    2    0    0 1000    0    7]\n",
      " [   4    0    2    3    3    2    2    3  951    4]\n",
      " [   2    2    0    5   10    3    0    5    1  981]]\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "print(\"accuracy:\", metrics.accuracy_score(y_true=y_val, y_pred=y_pred), \"\\n\")\n",
    "\n",
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_true=y_val, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
