{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.4.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset-4/household_power_consumption.txt', sep=';', low_memory=False, na_values=['nan','?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Global_active_power']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0] = df.iloc[:,0].fillna(df.iloc[:,0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_active_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Global_active_power\n",
       "0                4.216\n",
       "1                5.360\n",
       "2                5.374\n",
       "3                5.388\n",
       "4                3.666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60, a[0]+1):\n",
    "    data.append(df.iloc[i-60:i,0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df.iloc[60:a[0]+1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389125749029633"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.57310589e-03, -3.21065458e-03, -1.10667268e-04, -1.27282133e-03,\n",
       "       -3.57005603e-04,  2.33058369e-03, -6.58377457e-04, -1.20640020e-03,\n",
       "        1.40212268e-03,  2.57095035e-03, -3.95237886e-03,  1.52086393e-03,\n",
       "       -8.12909035e-04, -2.18253951e-03, -3.84794908e-04,  3.48074869e-03,\n",
       "        3.37032894e-03, -6.69151765e-04,  3.65954637e-03,  2.49591470e-03,\n",
       "       -3.06530523e-03, -1.82434959e-06, -9.84578896e-04, -4.18009936e-04,\n",
       "        7.21904608e-04,  1.85299210e-03,  2.69574516e-03, -3.53533885e-03,\n",
       "       -1.24381624e-03,  2.23041410e-03,  9.03947956e-04, -2.53621766e-03,\n",
       "        2.61597689e-04,  1.18694579e-03,  1.13953247e-04, -1.99670003e-03,\n",
       "        4.29409968e-03, -4.12373094e-03,  4.55037796e-03,  4.28222283e-03,\n",
       "       -1.22802420e-02,  3.55948556e-03,  1.37135887e-02,  8.25015453e-03,\n",
       "        4.85110578e-03,  4.00208750e-03,  6.27591046e-04, -3.68888603e-03,\n",
       "        3.45902088e-03, -5.12212250e-03,  4.22058598e-03, -4.52799571e-03,\n",
       "       -1.51837332e-02, -3.81683958e-02,  5.80598689e-02,  2.05420278e-02,\n",
       "        4.17159164e-02,  1.24598338e-02, -1.36314084e-01,  1.00365636e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06741620938643988"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=60, out_features=40, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=40, out_features=20, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 60\n",
    "hidden_sizes = [40, 20]\n",
    "output_size = 1\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = map(torch.tensor, (x_train, x_val, y_train, y_val))\n",
    "x_train = x_train.type(torch.FloatTensor)\n",
    "y_train = y_train.type(torch.FloatTensor)\n",
    "x_val = x_val.type(torch.FloatTensor)\n",
    "y_val = y_val.type(torch.FloatTensor)\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_interval=20000):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each batch from the training set\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.view(data.shape[0], -1)\n",
    "        # Copy data to GPU if needed\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Zero gradient buffers\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # Pass data through the network\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(output, target)\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loss_vector, accuracy_vector):\n",
    "    pred_arr = []\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    for data, target in val_loader:\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        val_loss += F.mse_loss(output, target).data.item()\n",
    "        pred = output.data.max(1)[0] # get the index of the max log-probability\n",
    "        for i in np.array(pred):\n",
    "            pred_arr.append(i)\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    loss_vector.append(val_loss)\n",
    "    \n",
    "    print('\\nValidation set: Average loss: {:.4f}\\n'.format(\n",
    "        val_loss))\n",
    "    \n",
    "    return pred_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1660159 (0%)]\tLoss: 0.130404\n",
      "Train Epoch: 1 [640000/1660159 (39%)]\tLoss: 0.027199\n",
      "Train Epoch: 1 [1280000/1660159 (77%)]\tLoss: 0.014153\n",
      "\n",
      "Validation set: Average loss: 0.0626\n",
      "\n",
      "Train Epoch: 2 [0/1660159 (0%)]\tLoss: 0.038056\n",
      "Train Epoch: 2 [640000/1660159 (39%)]\tLoss: 0.013188\n",
      "Train Epoch: 2 [1280000/1660159 (77%)]\tLoss: 0.011561\n",
      "\n",
      "Validation set: Average loss: 0.0621\n",
      "\n",
      "Train Epoch: 3 [0/1660159 (0%)]\tLoss: 0.036577\n",
      "Train Epoch: 3 [640000/1660159 (39%)]\tLoss: 0.143138\n",
      "Train Epoch: 3 [1280000/1660159 (77%)]\tLoss: 0.011874\n",
      "\n",
      "Validation set: Average loss: 0.0614\n",
      "\n",
      "Train Epoch: 4 [0/1660159 (0%)]\tLoss: 0.122412\n",
      "Train Epoch: 4 [640000/1660159 (39%)]\tLoss: 0.014628\n",
      "Train Epoch: 4 [1280000/1660159 (77%)]\tLoss: 0.037135\n",
      "\n",
      "Validation set: Average loss: 0.0630\n",
      "\n",
      "Train Epoch: 5 [0/1660159 (0%)]\tLoss: 0.103690\n",
      "Train Epoch: 5 [640000/1660159 (39%)]\tLoss: 0.045446\n",
      "Train Epoch: 5 [1280000/1660159 (77%)]\tLoss: 0.012782\n",
      "\n",
      "Validation set: Average loss: 0.0600\n",
      "\n",
      "Train Epoch: 6 [0/1660159 (0%)]\tLoss: 0.040755\n",
      "Train Epoch: 6 [640000/1660159 (39%)]\tLoss: 0.034850\n",
      "Train Epoch: 6 [1280000/1660159 (77%)]\tLoss: 0.019612\n",
      "\n",
      "Validation set: Average loss: 0.0612\n",
      "\n",
      "Train Epoch: 7 [0/1660159 (0%)]\tLoss: 0.119588\n",
      "Train Epoch: 7 [640000/1660159 (39%)]\tLoss: 0.003428\n",
      "Train Epoch: 7 [1280000/1660159 (77%)]\tLoss: 0.141765\n",
      "\n",
      "Validation set: Average loss: 0.0644\n",
      "\n",
      "Train Epoch: 8 [0/1660159 (0%)]\tLoss: 0.211412\n",
      "Train Epoch: 8 [640000/1660159 (39%)]\tLoss: 0.113606\n",
      "Train Epoch: 8 [1280000/1660159 (77%)]\tLoss: 0.037486\n",
      "\n",
      "Validation set: Average loss: 0.0602\n",
      "\n",
      "Train Epoch: 9 [0/1660159 (0%)]\tLoss: 0.073469\n",
      "Train Epoch: 9 [640000/1660159 (39%)]\tLoss: 0.015832\n",
      "Train Epoch: 9 [1280000/1660159 (77%)]\tLoss: 0.054559\n",
      "\n",
      "Validation set: Average loss: 0.0618\n",
      "\n",
      "Train Epoch: 10 [0/1660159 (0%)]\tLoss: 0.221374\n",
      "Train Epoch: 10 [640000/1660159 (39%)]\tLoss: 0.074724\n",
      "Train Epoch: 10 [1280000/1660159 (77%)]\tLoss: 0.043197\n",
      "\n",
      "Validation set: Average loss: 0.0650\n",
      "\n",
      "CPU times: user 7min 31s, sys: 6.58 s, total: 7min 37s\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 10\n",
    "\n",
    "lossv, accv = [], []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    output = validate(lossv, accv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.70004773,\n",
       " 3.38912,\n",
       " 0.30652094,\n",
       " 0.15509534,\n",
       " 0.46095562,\n",
       " 2.283328,\n",
       " 1.3422738,\n",
       " 0.22774863,\n",
       " 0.27269435,\n",
       " 0.18676448,\n",
       " 1.2484523,\n",
       " 0.25082445,\n",
       " 2.352014,\n",
       " 0.37089396,\n",
       " 1.5387925,\n",
       " 1.1215504,\n",
       " 3.6695535,\n",
       " 0.22235775,\n",
       " 0.38361168,\n",
       " 0.23114944,\n",
       " 0.41361475,\n",
       " 0.37262297,\n",
       " 0.28131437,\n",
       " 0.41496825,\n",
       " 0.35933018,\n",
       " 0.3880906,\n",
       " 4.984726,\n",
       " 0.65875053,\n",
       " 2.812698,\n",
       " 0.7650869,\n",
       " 0.32336116,\n",
       " 1.345638,\n",
       " 0.30750155,\n",
       " 0.60729504,\n",
       " 2.074424,\n",
       " 1.4414909,\n",
       " 0.31206703,\n",
       " 1.5265895,\n",
       " 0.25923324,\n",
       " 0.21708417,\n",
       " 5.392077,\n",
       " 2.8410826,\n",
       " 0.25618148,\n",
       " 1.5952063,\n",
       " 3.5725968,\n",
       " 1.1215504,\n",
       " 1.4235463,\n",
       " 0.64829516,\n",
       " 1.5982091,\n",
       " 0.53437495,\n",
       " 1.3427075,\n",
       " 0.65421057,\n",
       " 1.6114732,\n",
       " 0.37749934,\n",
       " 0.28109598,\n",
       " 4.3897004,\n",
       " 0.34672332,\n",
       " 0.44455624,\n",
       " 0.3593483,\n",
       " 0.24200487,\n",
       " 0.38367152,\n",
       " 0.33628178,\n",
       " 0.17495918,\n",
       " 1.4168779,\n",
       " 1.6173584,\n",
       " 0.5068166,\n",
       " 0.48180914,\n",
       " 1.9818237,\n",
       " 0.3676083,\n",
       " 0.31402373,\n",
       " 1.6130815,\n",
       " 0.25170612,\n",
       " 0.13665152,\n",
       " 1.5682175,\n",
       " 0.42132807,\n",
       " 1.6107242,\n",
       " 0.789726,\n",
       " 0.35903597,\n",
       " 1.6672701,\n",
       " 2.9260256,\n",
       " 3.6928735,\n",
       " 0.34208012,\n",
       " 1.362069,\n",
       " 0.37016773,\n",
       " 0.22326183,\n",
       " 1.1159894,\n",
       " 0.4264388,\n",
       " 1.4976634,\n",
       " 0.40274405,\n",
       " 0.277354,\n",
       " 1.5701499,\n",
       " 4.5447774,\n",
       " 2.202993,\n",
       " 0.4679327,\n",
       " 0.6153779,\n",
       " 0.3312366,\n",
       " 2.210939,\n",
       " 1.6139641,\n",
       " 4.798543,\n",
       " 0.3466313,\n",
       " 1.7871783,\n",
       " 0.31616616,\n",
       " 1.9384179,\n",
       " 0.47504163,\n",
       " 4.797645,\n",
       " 4.6344795,\n",
       " 0.21995735,\n",
       " 1.2942129,\n",
       " 0.3686757,\n",
       " 1.7605928,\n",
       " 1.8260626,\n",
       " 0.3471563,\n",
       " 0.6092715,\n",
       " 5.641183,\n",
       " 0.3448205,\n",
       " 0.2307086,\n",
       " 1.481538,\n",
       " 0.13119984,\n",
       " 1.8341349,\n",
       " 4.325452,\n",
       " 2.5563388,\n",
       " 0.9736322,\n",
       " 2.0037086,\n",
       " 0.26811767,\n",
       " 1.3242089,\n",
       " 0.9036403,\n",
       " 1.4733919,\n",
       " 0.2528627,\n",
       " 0.69402623,\n",
       " 1.5792781,\n",
       " 0.7102034,\n",
       " 0.2794528,\n",
       " 0.24644876,\n",
       " 0.37337446,\n",
       " 0.39223742,\n",
       " 3.2007306,\n",
       " 1.4857682,\n",
       " 0.3252883,\n",
       " 1.6010808,\n",
       " 1.1305751,\n",
       " 0.32380486,\n",
       " 0.24297595,\n",
       " 2.0885172,\n",
       " 0.34133244,\n",
       " 1.1893628,\n",
       " 0.30019975,\n",
       " 1.8821756,\n",
       " 2.1046715,\n",
       " 0.38972282,\n",
       " 0.71361136,\n",
       " 4.3348556,\n",
       " 3.1708052,\n",
       " 0.4188044,\n",
       " 0.29635143,\n",
       " 0.47364187,\n",
       " 1.2924079,\n",
       " 0.9353647,\n",
       " 0.25957847,\n",
       " 0.8768797,\n",
       " 1.9969511,\n",
       " 2.501278,\n",
       " 0.3528397,\n",
       " 0.61154246,\n",
       " 0.28301144,\n",
       " 2.4572563,\n",
       " 0.8879237,\n",
       " 0.46514273,\n",
       " 0.35239244,\n",
       " 0.43168736,\n",
       " 0.3160386,\n",
       " 0.55973625,\n",
       " 1.8327589,\n",
       " 0.29288173,\n",
       " 5.300916,\n",
       " 3.5810702,\n",
       " 0.49481583,\n",
       " 0.3556614,\n",
       " 0.73036504,\n",
       " 0.7961192,\n",
       " 6.181057,\n",
       " 0.21325421,\n",
       " 0.29511642,\n",
       " 0.8510423,\n",
       " 1.5527055,\n",
       " 0.17081094,\n",
       " 1.7695719,\n",
       " 0.45290804,\n",
       " 0.33853054,\n",
       " 1.5697517,\n",
       " 0.9037852,\n",
       " 5.719513,\n",
       " 1.2609885,\n",
       " 0.31535006,\n",
       " 1.7973306,\n",
       " 0.1399312,\n",
       " 0.21683264,\n",
       " 0.1546576,\n",
       " 0.3145051,\n",
       " 1.3728014,\n",
       " 2.6089149,\n",
       " 1.4029814,\n",
       " 0.32988214,\n",
       " 0.24085045,\n",
       " 0.27191377,\n",
       " 0.50611496,\n",
       " 0.30008626,\n",
       " 1.7049955,\n",
       " 0.35708356,\n",
       " 0.90264344,\n",
       " 1.7955188,\n",
       " 0.3334856,\n",
       " 0.38094306,\n",
       " 0.68300056,\n",
       " 4.22746,\n",
       " 0.3028674,\n",
       " 1.6057949,\n",
       " 1.707533,\n",
       " 0.74662447,\n",
       " 0.17184448,\n",
       " 0.45805597,\n",
       " 1.2520603,\n",
       " 1.539377,\n",
       " 1.4413679,\n",
       " 0.33118224,\n",
       " 1.7118866,\n",
       " 0.21777153,\n",
       " 0.18195772,\n",
       " 0.31968737,\n",
       " 0.27723312,\n",
       " 1.8503656,\n",
       " 0.6781912,\n",
       " 0.9143443,\n",
       " 1.3520857,\n",
       " 1.869733,\n",
       " 2.036311,\n",
       " 0.8175678,\n",
       " 2.0420463,\n",
       " 0.16936207,\n",
       " 0.16902137,\n",
       " 0.33512688,\n",
       " 0.43310785,\n",
       " 0.32218337,\n",
       " 0.59152937,\n",
       " 4.6911607,\n",
       " 0.4229715,\n",
       " 0.37718987,\n",
       " 0.9814979,\n",
       " 5.790319,\n",
       " 0.3926046,\n",
       " 1.5007148,\n",
       " 1.3197002,\n",
       " 3.3242474,\n",
       " 1.5603981,\n",
       " 0.28221893,\n",
       " 1.3302583,\n",
       " 0.47478414,\n",
       " 0.41069365,\n",
       " 1.2922208,\n",
       " 1.4355996,\n",
       " 3.607205,\n",
       " 0.191782,\n",
       " 1.3296186,\n",
       " 1.3281317,\n",
       " 1.2828425,\n",
       " 0.44395065,\n",
       " 0.4142475,\n",
       " 9.246143,\n",
       " 2.423051,\n",
       " 1.9250232,\n",
       " 0.29238296,\n",
       " 1.4037957,\n",
       " 1.5255612,\n",
       " 0.29999208,\n",
       " 1.7082562,\n",
       " 1.9984988,\n",
       " 0.725883,\n",
       " 0.37241364,\n",
       " 1.6641726,\n",
       " 0.42945695,\n",
       " 0.6411948,\n",
       " 1.3722978,\n",
       " 1.2438072,\n",
       " 0.34364247,\n",
       " 0.25934482,\n",
       " 2.8291314,\n",
       " 0.2590239,\n",
       " 0.26375413,\n",
       " 2.6689787,\n",
       " 0.53123116,\n",
       " 2.6273344,\n",
       " 0.38921094,\n",
       " 0.19051695,\n",
       " 0.44446468,\n",
       " 1.5568894,\n",
       " 0.42019677,\n",
       " 1.4831668,\n",
       " 1.6799936,\n",
       " 1.453707,\n",
       " 0.33790994,\n",
       " 0.7584064,\n",
       " 0.2968061,\n",
       " 0.66003823,\n",
       " 2.1744056,\n",
       " 3.6506827,\n",
       " 1.7174071,\n",
       " 0.2560506,\n",
       " 1.4817398,\n",
       " 1.4626211,\n",
       " 0.37350678,\n",
       " 0.25652862,\n",
       " 0.250448,\n",
       " 0.27529955,\n",
       " 0.3782568,\n",
       " 0.38865328,\n",
       " 0.47470427,\n",
       " 0.11975384,\n",
       " 0.53300834,\n",
       " 0.98742056,\n",
       " 0.4160986,\n",
       " 1.3285925,\n",
       " 2.2856622,\n",
       " 0.5849459,\n",
       " 0.29861212,\n",
       " 2.603173,\n",
       " 1.539533,\n",
       " 1.5903087,\n",
       " 0.34471154,\n",
       " 0.7797947,\n",
       " 0.19726658,\n",
       " 4.55521,\n",
       " 0.24253678,\n",
       " 0.14281535,\n",
       " 0.35322475,\n",
       " 3.0481744,\n",
       " 3.998845,\n",
       " 0.36018586,\n",
       " 2.6812196,\n",
       " 1.7759426,\n",
       " 1.1215504,\n",
       " 0.29363465,\n",
       " 2.1152365,\n",
       " 1.1227549,\n",
       " 2.9171236,\n",
       " 0.46376514,\n",
       " 0.19113374,\n",
       " 0.35241437,\n",
       " 0.35613918,\n",
       " 0.39446306,\n",
       " 1.9097931,\n",
       " 0.28614187,\n",
       " 0.28599572,\n",
       " 0.6287675,\n",
       " 1.5447534,\n",
       " 3.333208,\n",
       " 2.9920647,\n",
       " 1.6412553,\n",
       " 0.24074864,\n",
       " 1.5595719,\n",
       " 0.2590351,\n",
       " 2.0447536,\n",
       " 2.5823843,\n",
       " 1.5657761,\n",
       " 1.3555629,\n",
       " 1.8075787,\n",
       " 1.5078464,\n",
       " 1.4516793,\n",
       " 0.6250949,\n",
       " 1.4617409,\n",
       " 3.492707,\n",
       " 1.5018325,\n",
       " 1.2249866,\n",
       " 0.8607433,\n",
       " 0.18785787,\n",
       " 1.2094845,\n",
       " 1.4315208,\n",
       " 0.90373564,\n",
       " 0.29557943,\n",
       " 2.3463476,\n",
       " 3.2311895,\n",
       " 0.504648,\n",
       " 0.5009911,\n",
       " 1.5699466,\n",
       " 2.3298917,\n",
       " 0.31006718,\n",
       " 1.557482,\n",
       " 0.45085287,\n",
       " 0.17328596,\n",
       " 0.356668,\n",
       " 3.3132064,\n",
       " 0.35364795,\n",
       " 2.9210851,\n",
       " 4.2353287,\n",
       " 0.3376112,\n",
       " 3.6802173,\n",
       " 0.30829668,\n",
       " 2.932881,\n",
       " 0.13515615,\n",
       " 2.2885828,\n",
       " 1.7187394,\n",
       " 1.3638047,\n",
       " 0.33929396,\n",
       " 3.2060685,\n",
       " 0.34292126,\n",
       " 0.23390245,\n",
       " 0.42488837,\n",
       " 1.410886,\n",
       " 0.3091209,\n",
       " 1.49277,\n",
       " 1.8025168,\n",
       " 1.463054,\n",
       " 1.6987795,\n",
       " 2.977103,\n",
       " 1.5366584,\n",
       " 1.1123211,\n",
       " 0.5244894,\n",
       " 2.6949952,\n",
       " 1.3071916,\n",
       " 0.32377553,\n",
       " 0.33409023,\n",
       " 1.7058122,\n",
       " 0.36235476,\n",
       " 0.35033464,\n",
       " 0.47578788,\n",
       " 3.9970481,\n",
       " 0.4418583,\n",
       " 0.42369843,\n",
       " 1.714675,\n",
       " 0.17840385,\n",
       " 0.48421288,\n",
       " 0.4982977,\n",
       " 3.6655207,\n",
       " 1.8086404,\n",
       " 3.2309089,\n",
       " 0.40099907,\n",
       " 0.33360457,\n",
       " 0.29440498,\n",
       " 0.6132512,\n",
       " 3.3556328,\n",
       " 0.21507406,\n",
       " 1.6683496,\n",
       " 0.2692337,\n",
       " 0.3112228,\n",
       " 1.357635,\n",
       " 0.4020853,\n",
       " 1.4621513,\n",
       " 0.9426522,\n",
       " 1.2895465,\n",
       " 0.3665216,\n",
       " 0.9083886,\n",
       " 2.5888786,\n",
       " 0.58815646,\n",
       " 0.18127728,\n",
       " 2.0532792,\n",
       " 0.26062918,\n",
       " 0.27782106,\n",
       " 1.4612464,\n",
       " 1.4688432,\n",
       " 1.4231052,\n",
       " 0.5411284,\n",
       " 0.28167605,\n",
       " 0.20713305,\n",
       " 0.9094615,\n",
       " 0.6404011,\n",
       " 0.2533815,\n",
       " 0.22592664,\n",
       " 1.7304171,\n",
       " 0.5053954,\n",
       " 3.5286658,\n",
       " 5.768432,\n",
       " 0.24872684,\n",
       " 1.3921497,\n",
       " 5.7942195,\n",
       " 2.421287,\n",
       " 0.26246095,\n",
       " 0.27450013,\n",
       " 0.64721155,\n",
       " 0.40721273,\n",
       " 0.2914722,\n",
       " 1.4687564,\n",
       " 3.5840726,\n",
       " 0.58670497,\n",
       " 1.8080945,\n",
       " 0.25650406,\n",
       " 0.23380947,\n",
       " 0.18383455,\n",
       " 0.2935624,\n",
       " 0.37986016,\n",
       " 1.142319,\n",
       " 0.3052833,\n",
       " 0.8196807,\n",
       " 0.35675025,\n",
       " 1.0906832,\n",
       " 0.39905453,\n",
       " 0.15289879,\n",
       " 0.5333407,\n",
       " 1.5040411,\n",
       " 0.47596097,\n",
       " 0.34408283,\n",
       " 0.58425903,\n",
       " 2.0319653,\n",
       " 1.3921205,\n",
       " 1.3275988,\n",
       " 0.8412118,\n",
       " 1.6605171,\n",
       " 0.35946417,\n",
       " 1.3771116,\n",
       " 3.0947983,\n",
       " 0.3800857,\n",
       " 0.37111712,\n",
       " 0.36570096,\n",
       " 1.533404,\n",
       " 0.23257327,\n",
       " 1.5249199,\n",
       " 0.608397,\n",
       " 0.29203677,\n",
       " 0.4501531,\n",
       " 1.5545378,\n",
       " 0.25234318,\n",
       " 0.27109027,\n",
       " 0.28259063,\n",
       " 0.25579858,\n",
       " 0.2780974,\n",
       " 1.4297875,\n",
       " 0.9964838,\n",
       " 1.9997077,\n",
       " 1.7276194,\n",
       " 0.44138646,\n",
       " 0.36024213,\n",
       " 0.5183866,\n",
       " 0.38385558,\n",
       " 1.4720393,\n",
       " 0.66817045,\n",
       " 0.37041187,\n",
       " 0.45340204,\n",
       " 1.4106419,\n",
       " 1.4909523,\n",
       " 3.8895776,\n",
       " 0.3564675,\n",
       " 3.858206,\n",
       " 0.68597317,\n",
       " 0.40677,\n",
       " 4.7210364,\n",
       " 2.9566,\n",
       " 0.2857895,\n",
       " 1.4540713,\n",
       " 0.41747594,\n",
       " 0.25492883,\n",
       " 1.8031508,\n",
       " 0.41297913,\n",
       " 0.28320408,\n",
       " 0.34281063,\n",
       " 1.8871197,\n",
       " 0.37855864,\n",
       " 0.3519144,\n",
       " 0.3809557,\n",
       " 0.601084,\n",
       " 1.3634875,\n",
       " 0.33373475,\n",
       " 0.23838782,\n",
       " 1.4590015,\n",
       " 0.38634324,\n",
       " 0.1751399,\n",
       " 0.23823166,\n",
       " 0.24397373,\n",
       " 0.44775772,\n",
       " 1.907816,\n",
       " 0.38326502,\n",
       " 0.26846647,\n",
       " 0.43992352,\n",
       " 0.2675202,\n",
       " 0.25410962,\n",
       " 1.7926042,\n",
       " 0.6794965,\n",
       " 1.3939577,\n",
       " 1.3053262,\n",
       " 1.2057263,\n",
       " 4.06901,\n",
       " 1.3416822,\n",
       " 2.2499752,\n",
       " 0.28507018,\n",
       " 0.71786785,\n",
       " 1.2585064,\n",
       " 0.17975593,\n",
       " 1.7965711,\n",
       " 0.2588024,\n",
       " 0.3641243,\n",
       " 1.924157,\n",
       " 4.2734184,\n",
       " 0.29218245,\n",
       " 0.3404312,\n",
       " 0.20666313,\n",
       " 1.390787,\n",
       " 0.23351455,\n",
       " 0.46383667,\n",
       " 0.48777485,\n",
       " 0.9710072,\n",
       " 2.5057073,\n",
       " 2.5059836,\n",
       " 2.228735,\n",
       " 0.30754805,\n",
       " 0.34190106,\n",
       " 2.1016724,\n",
       " 0.33746862,\n",
       " 0.32883453,\n",
       " 1.957192,\n",
       " 0.2724316,\n",
       " 0.551337,\n",
       " 1.3401754,\n",
       " 0.27778268,\n",
       " 0.35080338,\n",
       " 0.28642488,\n",
       " 1.992583,\n",
       " 1.7574754,\n",
       " 1.4268205,\n",
       " 0.38428974,\n",
       " 1.5074716,\n",
       " 0.27866173,\n",
       " 0.32954025,\n",
       " 2.1099858,\n",
       " 1.689763,\n",
       " 0.94726825,\n",
       " 1.403768,\n",
       " 1.3067292,\n",
       " 0.67346525,\n",
       " 0.32576466,\n",
       " 0.49260497,\n",
       " 2.8510485,\n",
       " 0.29820275,\n",
       " 0.32059622,\n",
       " 1.5341642,\n",
       " 0.47030973,\n",
       " 2.6172445,\n",
       " 2.8823028,\n",
       " 0.24932933,\n",
       " 2.722317,\n",
       " 0.907722,\n",
       " 1.5335655,\n",
       " 0.22600412,\n",
       " 2.2134945,\n",
       " 0.3330698,\n",
       " 0.31512094,\n",
       " 0.17193222,\n",
       " 1.700844,\n",
       " 0.34881973,\n",
       " 0.25844312,\n",
       " 0.54005504,\n",
       " 1.3855985,\n",
       " 0.22654104,\n",
       " 0.41433287,\n",
       " 0.19090366,\n",
       " 0.2790997,\n",
       " 0.42540717,\n",
       " 0.3311417,\n",
       " 1.3276513,\n",
       " 5.8921375,\n",
       " 0.48752594,\n",
       " 0.28650856,\n",
       " 0.22015715,\n",
       " 3.97466,\n",
       " 3.5577765,\n",
       " 3.3996444,\n",
       " 1.5791297,\n",
       " 0.3208294,\n",
       " 0.38498545,\n",
       " 0.365283,\n",
       " 0.32863188,\n",
       " 0.38776636,\n",
       " 1.1116598,\n",
       " 0.85415864,\n",
       " 1.5412033,\n",
       " 2.2534869,\n",
       " 0.3112998,\n",
       " 0.24398875,\n",
       " 0.367234,\n",
       " 1.2468241,\n",
       " 0.24615526,\n",
       " 1.6524127,\n",
       " 2.2654824,\n",
       " 0.38198662,\n",
       " 1.6648797,\n",
       " 0.24145961,\n",
       " 0.2507925,\n",
       " 0.2853856,\n",
       " 0.2613957,\n",
       " 2.3041847,\n",
       " 0.24807286,\n",
       " 0.38593054,\n",
       " 0.23007989,\n",
       " 0.2467339,\n",
       " 0.47547603,\n",
       " 1.777024,\n",
       " 0.3445766,\n",
       " 1.4257282,\n",
       " 0.2791288,\n",
       " 2.8287604,\n",
       " 0.26846504,\n",
       " 1.945009,\n",
       " 0.49391317,\n",
       " 0.403198,\n",
       " 0.4616778,\n",
       " 1.2580986,\n",
       " 0.252141,\n",
       " 1.4749615,\n",
       " 1.8236916,\n",
       " 0.56160593,\n",
       " 0.9645363,\n",
       " 0.9652711,\n",
       " 4.1775856,\n",
       " 2.9202294,\n",
       " 0.35498,\n",
       " 2.0464325,\n",
       " 0.36312366,\n",
       " 1.5145798,\n",
       " 0.30861998,\n",
       " 0.52794003,\n",
       " 0.4299295,\n",
       " 1.6426251,\n",
       " 1.3094493,\n",
       " 2.749261,\n",
       " 1.1215504,\n",
       " 0.3189361,\n",
       " 0.20270944,\n",
       " 1.9091375,\n",
       " 0.3344252,\n",
       " 0.5978019,\n",
       " 2.6506908,\n",
       " 0.30501437,\n",
       " 2.7886043,\n",
       " 0.3355527,\n",
       " 2.4249291,\n",
       " 1.2582084,\n",
       " 1.5895439,\n",
       " 0.17329788,\n",
       " 0.22026205,\n",
       " 1.7501357,\n",
       " 0.48438263,\n",
       " 2.0820553,\n",
       " 0.3541491,\n",
       " 0.348063,\n",
       " 4.6156664,\n",
       " 0.3556218,\n",
       " 0.8236768,\n",
       " 2.5368626,\n",
       " 0.40508032,\n",
       " 0.23509192,\n",
       " 1.3296801,\n",
       " 0.7583177,\n",
       " 0.28005457,\n",
       " 1.4392587,\n",
       " 0.89864993,\n",
       " 0.2295742,\n",
       " 0.35726285,\n",
       " 0.592793,\n",
       " 0.26600766,\n",
       " 0.31982374,\n",
       " 0.4733317,\n",
       " 2.1911354,\n",
       " 0.40413165,\n",
       " 0.3316064,\n",
       " 3.953711,\n",
       " 0.3291278,\n",
       " 1.353263,\n",
       " 1.6887946,\n",
       " 0.27121973,\n",
       " 0.3160951,\n",
       " 0.45528245,\n",
       " 1.4910756,\n",
       " 0.6374941,\n",
       " 0.19502592,\n",
       " 0.4452107,\n",
       " 1.5191228,\n",
       " 1.7003124,\n",
       " 0.7448952,\n",
       " 0.5154972,\n",
       " 0.19994068,\n",
       " 1.0398015,\n",
       " 0.99639094,\n",
       " 1.5661819,\n",
       " 1.3286291,\n",
       " 1.2735558,\n",
       " 1.5642414,\n",
       " 0.33117223,\n",
       " 0.26993155,\n",
       " 0.40223217,\n",
       " 5.2542486,\n",
       " 0.5785899,\n",
       " 0.28919387,\n",
       " 0.26819062,\n",
       " 1.7664564,\n",
       " 0.25223637,\n",
       " 0.46330976,\n",
       " 0.34401822,\n",
       " 0.43482828,\n",
       " 2.6835523,\n",
       " 1.7354318,\n",
       " 0.42807245,\n",
       " 1.4321873,\n",
       " 0.6786971,\n",
       " 1.9284245,\n",
       " 1.1728089,\n",
       " 0.35371614,\n",
       " 0.39044285,\n",
       " 2.092289,\n",
       " 1.1215504,\n",
       " 0.43654656,\n",
       " 0.33734345,\n",
       " 0.29138398,\n",
       " 0.34722042,\n",
       " 0.39637995,\n",
       " 0.19892836,\n",
       " 3.8505611,\n",
       " 0.4826119,\n",
       " 2.9568284,\n",
       " 0.43892503,\n",
       " 0.5377624,\n",
       " 3.2666132,\n",
       " 4.176381,\n",
       " 1.5571284,\n",
       " 0.34082603,\n",
       " 1.3936889,\n",
       " 0.45199823,\n",
       " 1.5295874,\n",
       " 1.5886447,\n",
       " 2.1700935,\n",
       " 0.32408428,\n",
       " 0.46155167,\n",
       " 0.2551763,\n",
       " 0.3348291,\n",
       " 0.4281962,\n",
       " 0.70946145,\n",
       " 1.7056614,\n",
       " 0.37427878,\n",
       " 1.4685278,\n",
       " 0.3441198,\n",
       " 0.4686613,\n",
       " 1.3754425,\n",
       " 0.14725709,\n",
       " 0.23289895,\n",
       " 0.33847666,\n",
       " 0.30422902,\n",
       " 0.47797823,\n",
       " 1.2952106,\n",
       " 1.8943545,\n",
       " 1.3363147,\n",
       " 1.1739374,\n",
       " 2.403967,\n",
       " 1.9503092,\n",
       " 2.0473495,\n",
       " 0.36598134,\n",
       " 1.1215504,\n",
       " 0.44670916,\n",
       " 2.7578957,\n",
       " 2.5642712,\n",
       " 0.21974468,\n",
       " 1.4364333,\n",
       " 0.33978176,\n",
       " 2.2953131,\n",
       " 0.33862948,\n",
       " 0.20611191,\n",
       " 1.9898257,\n",
       " 1.485485,\n",
       " 0.3598621,\n",
       " 0.30363536,\n",
       " 8.300823,\n",
       " 3.6903734,\n",
       " 0.33029056,\n",
       " 0.28338623,\n",
       " 0.23473454,\n",
       " 2.4762034,\n",
       " 0.48989153,\n",
       " 0.15058184,\n",
       " 0.5975909,\n",
       " 1.4890432,\n",
       " 0.4369681,\n",
       " 0.40016222,\n",
       " 0.17363286,\n",
       " 0.3033626,\n",
       " 0.34372997,\n",
       " 0.27869368,\n",
       " 1.3505396,\n",
       " 0.33986068,\n",
       " 2.6042526,\n",
       " 0.31489015,\n",
       " 0.31021595,\n",
       " 0.3669622,\n",
       " 0.551939,\n",
       " 1.466019,\n",
       " 1.2191786,\n",
       " 0.3702135,\n",
       " 1.3578327,\n",
       " 0.33398175,\n",
       " 0.3937192,\n",
       " 0.42452168,\n",
       " 1.6510082,\n",
       " 1.6174262,\n",
       " 1.337527,\n",
       " 0.21187234,\n",
       " 3.3776236,\n",
       " 0.45069432,\n",
       " 1.3788414,\n",
       " 0.42295957,\n",
       " 2.4382868,\n",
       " 1.5320691,\n",
       " 0.34145737,\n",
       " 0.44892907,\n",
       " 0.5180669,\n",
       " 0.48227167,\n",
       " 0.44038105,\n",
       " 0.2148478,\n",
       " 1.8635072,\n",
       " 0.34578776,\n",
       " 0.3145888,\n",
       " 0.35847187,\n",
       " 3.7982092,\n",
       " 2.1198678,\n",
       " 0.2224381,\n",
       " 0.15590882,\n",
       " 1.7120358,\n",
       " 0.14811516,\n",
       " 2.4392946,\n",
       " 0.2467103,\n",
       " 1.5897509,\n",
       " 0.20513964,\n",
       " 0.21760845,\n",
       " 2.0759838,\n",
       " 0.4035797,\n",
       " 0.2580886,\n",
       " 0.90547156,\n",
       " 1.499414,\n",
       " 0.45448065,\n",
       " 0.31737733,\n",
       " 4.375614,\n",
       " 0.36277628,\n",
       " 1.3725408,\n",
       " 1.578614,\n",
       " 0.6776314,\n",
       " 1.4963973,\n",
       " 0.4208367,\n",
       " 0.8653047,\n",
       " 0.44808125,\n",
       " 1.5732737,\n",
       " 0.29021406,\n",
       " 1.2322404,\n",
       " 3.0079734,\n",
       " 3.6941426,\n",
       " 0.31942368,\n",
       " 0.17821145,\n",
       " 0.34367895,\n",
       " 0.5379839,\n",
       " 1.552127,\n",
       " 0.33324766,\n",
       " 0.36281037,\n",
       " 1.2693816,\n",
       " 0.26867533,\n",
       " 3.4349089,\n",
       " 0.38895702,\n",
       " 1.3868757,\n",
       " 2.318986,\n",
       " 0.4928379,\n",
       " 1.4754939,\n",
       " 0.2404716,\n",
       " 1.3102006,\n",
       " 1.5389788,\n",
       " 5.2052746,\n",
       " 0.5335095,\n",
       " 0.27430964,\n",
       " 3.708129,\n",
       " 0.39755845,\n",
       " 5.114005,\n",
       " 1.7671882,\n",
       " 0.17526484,\n",
       " 1.5282842,\n",
       " 1.1713896,\n",
       " 0.8296454,\n",
       " 1.530922,\n",
       " 2.7593243,\n",
       " 0.79660106,\n",
       " 2.509444,\n",
       " 1.8430579,\n",
       " 0.24773169,\n",
       " 2.0154028,\n",
       " 0.5183604,\n",
       " 0.27018547,\n",
       " 0.5368204,\n",
       " 0.3321283,\n",
       " 0.6618302,\n",
       " 0.56109905,\n",
       " 2.4561238,\n",
       " 1.4163462,\n",
       " 0.2676127,\n",
       " 1.1215506,\n",
       " 1.3707659,\n",
       " 0.6029639,\n",
       " 0.35776234,\n",
       " 1.5350561,\n",
       " 0.35177898,\n",
       " 4.619242,\n",
       " 0.54530907,\n",
       " 1.3463297,\n",
       " 0.18970251,\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
